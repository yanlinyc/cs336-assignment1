[tuning]
enabled = true
output_dir = "output/tuning"
num_tune_samples = 20
gpus_per_trial = 0.33
num_cpus_per_trial = 4
max_iterations = 10

[training]
output_dir = "output/checkpoints"
context_length = 256
train_batch_size = 32
eval_batch_size = 32
num_iterations = 5000
eval_num_batches = 200
save_steps = -1
eval_steps = 400
logging_steps = 200
device = "cuda"
debug_fixed_minibatch = false
random_seed = 42

[model]
vocab_size = 10000
context_length = 256
num_layers = 4
d_model = 512
num_heads = 16
d_ff = 1344
rope_theta = 10000.0

[optimizer]
weight_decay = 0.01
betas = [0.9, 0.999]
eps = 1e-9

[optimizer.lr_scheduler_config]
cls = "CosineLRScheduler"
warmup_iters = 100
cosine_cycle_iters = 4000
min_lr = 1e-5
