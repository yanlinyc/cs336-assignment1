[training]
output_dir = "output/checkpoints"
context_length = 256
train_batch_size = 32
eval_batch_size = 512
num_iterations = 40000
# eval_num_batches = 5465963 / context_length / eval_batch_size
eval_num_batches = 40
save_steps = 10000
eval_steps = 2000
logging_steps = 400
device = "cuda"
random_seed = 42

[model]
vocab_size = 10000
context_length = 256
num_layers = 4
d_model = 512
num_heads = 16
d_ff = 1344
rope_theta = 10000.0

[optimizer]
weight_decay = 0.01
betas = [0.9, 0.999]
eps = 1e-9

[optimizer.lr_scheduler_config]
cls = "CosineLRScheduler"
# 1% to 5% of the total training steps
warmup_iters = 500
# same as num_iterations
cosine_cycle_iters = 40000
min_lr = 1e-5
max_lr = 1e-3
