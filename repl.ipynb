{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f4d38b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b10b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.tokenizer.bpe import train_bpe_tokenizer\n",
    "from cs336_basics.tokenizer.bpe_optim import train_bpe_tokenizer_optim\n",
    "from cs336_basics.utils import save_pickle, load_pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc140d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d79a72",
   "metadata": {},
   "source": [
    "# REPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aa293f",
   "metadata": {},
   "source": [
    "As of Unicode 16.0 (released in September 2024), the standard defines 154,998 characters across 168 scripts\n",
    "```python\n",
    ">>> test_string = \"hello! こんにちは!\"\n",
    ">>> utf8_encoded = test_string.encode(\"utf-8\")\n",
    ">>> print(utf8_encoded)\n",
    "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
    ">>> print(type(utf8_encoded))\n",
    "<class 'bytes'>\n",
    ">>> # Get the byte values for the encoded string (integers from 0 to 255).\n",
    ">>> list(utf8_encoded)\n",
    "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129,\n",
    "161, 227, 129, 175, 33]\n",
    ">>> # One byte does not necessarily correspond to one Unicode character!\n",
    ">>> print(len(test_string))\n",
    "13\n",
    ">>> print(len(utf8_encoded))\n",
    "23\n",
    ">>> print(utf8_encoded.decode(\"utf-8\"))\n",
    "hello! こんにちは!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57915604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n",
      "29275\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")\n",
    "print(ord(\"牛\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4110509",
   "metadata": {},
   "source": [
    "When using byte-level tokenization, we do not need to worry about out-of-vocabulary tokens, since we know that any input text can be expressed as a sequence of integers from 0 to\n",
    "255 by converting our Unicode codepoints into a sequence of bytes (e.g., via the UTF-8 encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6778a2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u don\\'t have to be scared of the loud dog, I\\'ll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\\n',\n",
       " '<|endoftext|>\\n',\n",
       " 'Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\\n',\n",
       " 'Tom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\\n',\n",
       " 'Sam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\\n',\n",
       " 'They went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear them. They were sad and scared, and they never got out of the pit.\\n',\n",
       " '<|endoftext|>\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Tom and Lily were playing with their toys in the living room. They liked to build towers and bridges with their blocks and cars. Tom was very proud of his tall tower. He wanted to make it even taller, so he reached for more blocks.\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\n",
      "\n",
      "<|endoftext|>\n",
      "\n",
      "Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
      "\n",
      "Tom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\n",
      "\n",
      "Sam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\n",
      "\n",
      "They went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear them. They were sad and scared, and they never got out of the pit.\n",
      "\n",
      "<|endoftext|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tom and Lily were playing with their toys in the living room. They liked to build towers and bridges with their blocks and cars. Tom was very proud of his tall tower. He wanted to make it even taller, so he reached for more blocks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/TinyStoriesV2-GPT4-valid.txt\", \"r\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "display(content[:10])\n",
    "\n",
    "for line in content[:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651150a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f210f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(\"hello! こんにちは!\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b25bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(b\"\\xe3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fa643a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some',\n",
       " ' text',\n",
       " ' that',\n",
       " ' i',\n",
       " \"'ll\",\n",
       " ' pre',\n",
       " '-',\n",
       " 'tokenize',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'and',\n",
       " ' this',\n",
       " ' is',\n",
       " ' a',\n",
       " ' test',\n",
       " ' 1234']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\\n\\nand this is a test 1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242618f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test\n",
      "prev_token: , next_token: \n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "if i > 0 and (prev_token := \"\", next_token := \"\"):\n",
    "    print(\"This is a test\")\n",
    "    print(f\"prev_token: {prev_token}, next_token: {next_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889e740",
   "metadata": {},
   "source": [
    "# Train BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((b\"as\", b\"t\") > (b\" .\", b\"..\"))\n",
    "a = b\"as\" + b\"t\"\n",
    "b = b\" .\" + b\"..\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c5085f",
   "metadata": {},
   "source": [
    "```bash\n",
    "# test / profile\n",
    "uv run cs336_basics/bpe.py --input_path=data/test.txt --vocab_size=270\n",
    "uv run cs336_basics/bpe.py --input_path=data/TinyStoriesV2-GPT4-valid.txt --vocab_size=300\n",
    "uv run cs336_basics/bpe.py --input_path=data/owt_valid.txt --vocab_size=300\n",
    "uv run cs336_basics/bpe.py --input_path=data/owt_train.txt --vocab_size=260\n",
    "uv run scalene cs336_basics/bpe.py --input_path=data/owt_train.txt --vocab_size=300 --output_dir=data/test --pre_tokens_path=data/output/owt_train-pre_tokens.pkl --profile=true\n",
    "\n",
    "# production\n",
    "uv run cs336_basics/bpe.py --input_path=data/TinyStoriesV2-GPT4-train.txt --vocab_size=10000\n",
    "uv run cs336_basics/bpe.py --input_path=data/owt_train.txt --vocab_size=32000 --pre_tokens_path=data/output/owt_train-pre_tokens.pkl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36074fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer on data/test.txt with vocab size 267, special tokens ['<|endoftext|>']...\n",
      "Pre-tokenizing input data...\n",
      "Chunking file of size 95 bytes into 1 chunks of size 67108864 bytes each.\n",
      "Using 60 processes for parallelizing pre-tokenization of 1 chunks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f2a9923cda48a7a29d4a445b2e86d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-tokenizing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenization took 0.19 seconds. Found 8 unique tokens.\n",
      "Pre-tokens saved to data/test/test-pre_tokens.pkl.\n",
      "Unique pre-tokens: 8\n",
      "First 10 unique pre-tokens: ['\\n', ' low', ' lower', ' newest', ' widest', 'low', 'lower', 'newest']\n",
      "Last 10 unique pre-tokens: ['\\n', ' low', ' lower', ' newest', ' widest', 'low', 'lower', 'newest']\n",
      "Random 10 unique pre-tokens: ['\\n', ' low', ' lower', ' newest', ' widest', 'low', 'lower', 'newest']\n",
      "Starting BPE training with 10 merges.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b712d90071b8466398d1daa6a2765171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training BPE:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Merge 1/10: (b's', b't')\n",
      "Current pairs:\n",
      "[((b's', b't'), 9), ((b'e', b's'), 9), ((b'w', b'e'), 8), ((b'o', b'w'), 7), ((b'l', b'o'), 7), ((b'n', b'e'), 6), ((b'e', b'w'), 6), ((b' ', b'n'), 5), ((b' ', b'l'), 5), ((b'w', b'i'), 3)]\n",
      "---\n",
      "Merge 2/10: (b'e', b'st')\n",
      "Current pairs:\n",
      "[((b'e', b'st'), 9), ((b'w', b'e'), 8), ((b'o', b'w'), 7), ((b'l', b'o'), 7), ((b'n', b'e'), 6), ((b'e', b'w'), 6), ((b' ', b'n'), 5), ((b' ', b'l'), 5), ((b'w', b'i'), 3), ((b'i', b'd'), 3)]\n",
      "---\n",
      "Merge 3/10: (b'o', b'w')\n",
      "Current pairs:\n",
      "[((b'o', b'w'), 7), ((b'l', b'o'), 7), ((b'w', b'est'), 6), ((b'n', b'e'), 6), ((b'e', b'w'), 6), ((b' ', b'n'), 5), ((b' ', b'l'), 5), ((b'w', b'i'), 3), ((b'i', b'd'), 3), ((b'd', b'est'), 3)]\n",
      "---\n",
      "Merge 4/10: (b'l', b'ow')\n",
      "Current pairs:\n",
      "[((b'l', b'ow'), 7), ((b'w', b'est'), 6), ((b'n', b'e'), 6), ((b'e', b'w'), 6), ((b' ', b'n'), 5), ((b' ', b'l'), 5), ((b'w', b'i'), 3), ((b'i', b'd'), 3), ((b'd', b'est'), 3), ((b' ', b'w'), 3)]\n",
      "---\n",
      "Merge 5/10: (b'w', b'est')\n",
      "Current pairs:\n",
      "[((b'w', b'est'), 6), ((b'n', b'e'), 6), ((b'e', b'w'), 6), ((b' ', b'n'), 5), ((b' ', b'low'), 5), ((b'w', b'i'), 3), ((b'i', b'd'), 3), ((b'd', b'est'), 3), ((b' ', b'w'), 3), ((b'low', b'e'), 2)]\n",
      "---\n",
      "Merge 6/10: (b'n', b'e')\n",
      "Current pairs:\n",
      "[((b'n', b'e'), 6), ((b'e', b'west'), 6), ((b' ', b'n'), 5), ((b' ', b'low'), 5), ((b'w', b'i'), 3), ((b'i', b'd'), 3), ((b'd', b'est'), 3), ((b' ', b'w'), 3), ((b'low', b'e'), 2), ((b'e', b'r'), 2)]\n",
      "---\n",
      "Merge 7/10: (b'ne', b'west')\n",
      "Current pairs:\n",
      "[((b'ne', b'west'), 6), ((b' ', b'ne'), 5), ((b' ', b'low'), 5), ((b'w', b'i'), 3), ((b'i', b'd'), 3), ((b'd', b'est'), 3), ((b' ', b'w'), 3), ((b'low', b'e'), 2), ((b'e', b'r'), 2)]\n",
      "---\n",
      "Merge 8/10: (b' ', b'newest')\n",
      "Current pairs:\n",
      "[((b' ', b'newest'), 5), ((b' ', b'low'), 5), ((b'w', b'i'), 3), ((b'i', b'd'), 3), ((b'd', b'est'), 3), ((b' ', b'w'), 3), ((b'low', b'e'), 2), ((b'e', b'r'), 2)]\n",
      "---\n",
      "Merge 9/10: (b' ', b'low')\n",
      "Current pairs:\n",
      "[((b' ', b'low'), 5), ((b'w', b'i'), 3), ((b'i', b'd'), 3), ((b'd', b'est'), 3), ((b' ', b'w'), 3), ((b'low', b'e'), 2), ((b'e', b'r'), 2)]\n",
      "---\n",
      "Merge 10/10: (b'w', b'i')\n",
      "Current pairs:\n",
      "[((b'w', b'i'), 3), ((b'i', b'd'), 3), ((b'd', b'est'), 3), ((b' ', b'w'), 3), ((b'e', b'r'), 2), ((b'low', b'e'), 1), ((b' low', b'e'), 1)]\n",
      "Saving vocabulary and merges to data/test/test-bpe.pkl...\n",
      "Vocab size: 257\n",
      "Number of merges: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(b's', b't'),\n",
       " (b'e', b'st'),\n",
       " (b'o', b'w'),\n",
       " (b'l', b'ow'),\n",
       " (b'w', b'est'),\n",
       " (b'n', b'e'),\n",
       " (b'ne', b'west'),\n",
       " (b' ', b'newest'),\n",
       " (b' ', b'low'),\n",
       " (b'w', b'i')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = 10_000\n",
    "vocab_size = 32_000\n",
    "vocab_size = 300\n",
    "name = \"TinyStoriesV2-GPT4-valid\"\n",
    "# name = \"TinyStoriesV2-GPT4-train\"\n",
    "# name = \"owt_train\"\n",
    "name = \"test\"\n",
    "\n",
    "vocab, merges = train_bpe_tokenizer_optim(\n",
    "    input_path=f\"data/{name}.txt\",\n",
    "    vocab_size=257 + 10,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    output_dir=\"data/test\",\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Number of merges: {len(merges)}\")\n",
    "display(merges[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48750865",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10000 data/TinyStoriesV2-GPT4-train.txt > data/TinyStoriesV2-GPT4-train.10k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "809108bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b7\u001b[?47h\u001b[?1h\u001b=\n",
      "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
      "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
      "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
      "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
      "And that's how Ben found an amazing vase in the store!\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
      "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his friend, the duck. \"Hi, Ollie!\" said the duck. \"Hi, duck!\" said Ollie. \"I need to hurry and catch fish for my family.\"\n",
      "While Ollie was catching fish, he found a big shiny stone. He thought, \"This is not a fish, but it is so pretty!\" Ollie took the shiny stone home to show his family. They all looked at the shiny stone and smiled. The shiny stone made everyone happy, and they forgot about the fish for dinner.\n",
      "<|endoftext|>\n",
      "\u001b[7mdata/TinyStoriesV2-GPT4-train.10k.txt\u001b[m\u001b[K\u0007\u001b[H\u001b[2J\u001b[H\u001b[H\u001b[2J\u001b[H\n",
      "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
      "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
      "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
      "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
      "And that's how Ben found an amazing vase in the store!\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
      "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his friend, the duck. \"Hi, Ollie!\" said the duck. \"Hi, duck!\" said Ollie. \"I need to hurry and catch fish for my family.\"\n",
      "While Ollie was catching fish, he found a big shiny stone. He thought, \"This is not a fish, but it is so pretty!\" Ollie took the shiny stone home to show his family. They all looked at the shiny stone and smiled. The shiny stone made everyone happy, and they forgot about the fish for dinner.\n",
      "<|endoftext|>\n",
      ":\u001b[K\u0007\u001b[H\u001b[2J\u001b[H\u001b[H\u001b[2J\u001b[H\n",
      "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
      "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
      "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
      "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
      "And that's how Ben found an amazing vase in the store!\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
      "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his friend, the duck. \"Hi, Ollie!\" said the duck. \"Hi, duck!\" said Ollie. \"I need to hurry and catch fish for my family.\"\n",
      "While Ollie was catching fish, he found a big shiny stone. He thought, \"This is not a fish, but it is so pretty!\" Ollie took the shiny stone home to show his family. They all looked at the shiny stone and smiled. The shiny stone made everyone happy, and they forgot about the fish for dinner.\n",
      "<|endoftext|>\n",
      ":\u001b[K"
     ]
    }
   ],
   "source": [
    "!less data/TinyStoriesV2-GPT4-train.10k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254af44",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0400e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\"hello\"[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1b527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0] * 5\n",
    "a[2:4] = [3]\n",
    "a\n",
    "\n",
    "sum([[\"a\", \"b\"], [\"c\", \"d\"]], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cca7935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6.6M\n",
      "407712 -rw-r--r--. 1 yanlin_chen yanlin_chen  1.5K Jun 28 22:06 address.txt\n",
      "407713 -rw-r--r--. 1 yanlin_chen yanlin_chen  130K Jun 28 22:06 corpus.en\n",
      "407714 -rw-r--r--. 1 yanlin_chen yanlin_chen   594 Jun 28 22:06 german.txt\n",
      "407715 -rw-r--r--. 1 yanlin_chen yanlin_chen  446K Jun 28 22:06 gpt2_merges.txt\n",
      "407716 -rw-r--r--. 1 yanlin_chen yanlin_chen 1018K Jun 28 22:06 gpt2_vocab.json\n",
      "407717 -rw-r--r--. 1 yanlin_chen yanlin_chen    23 Jun 28 22:06 special_token_double_newlines_non_whitespace.txt\n",
      "407718 -rw-r--r--. 1 yanlin_chen yanlin_chen    15 Jun 28 22:06 special_token_trailing_newlines.txt\n",
      "407720 -rw-r--r--. 1 yanlin_chen yanlin_chen  5.0M Jun 28 22:06 tinystories_sample_5M.txt\n",
      "407719 -rw-r--r--. 1 yanlin_chen yanlin_chen  3.8K Jun 28 22:06 tinystories_sample.txt\n",
      "407721 -rw-r--r--. 1 yanlin_chen yanlin_chen  1.3K Jun 28 22:06 train-bpe-reference-merges.txt\n",
      "407722 -rw-r--r--. 1 yanlin_chen yanlin_chen  7.5K Jun 28 22:06 train-bpe-reference-vocab.json\n",
      "407723 drwxr-sr-x. 2 yanlin_chen yanlin_chen  4.0K Jun 28 22:06 ts_tests\n"
     ]
    }
   ],
   "source": [
    "!ls -lhi tests/fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67a877bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xf0\\x9f\\x99\\x83'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"🙃\"\n",
    "test_string.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xef\\xbf\\xbdhello'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\ufffd\".encode(\"utf-8\") + \"hello\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5516144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\u'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\u'\n",
      "/tmp/ipykernel_15972/744379407.py:1: SyntaxWarning: invalid escape sequence '\\u'\n",
      "  (b\"\\ufffd\" + \"hello\".encode(\"utf-8\")).decode(\"utf-8\", errors=\"replace\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\\\ufffdhello'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(b\"\\ufffd\" + \"hello\".encode(\"utf-8\")).decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0affbc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some text ',\n",
       " '<|endoftext|><|endoftext|>',\n",
       " ' another text',\n",
       " '<|endoftext|>',\n",
       " ' ',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "pattern = \"|\".join(re.escape(token) for token in [\"<|endoftext|><|endoftext|>\", \"<|endoftext|>\"])\n",
    "parts = re.split(\n",
    "    f\"({pattern})\",\n",
    "    \"some text <|endoftext|><|endoftext|> another text<|endoftext|> <|endoftext|>\",\n",
    ")\n",
    "[part for part in parts if part]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b31c896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "c[tuple(b.to_bytes() for b in \"hello\".encode(\"utf-8\"))] += 1\n",
    "c\n",
    "\n",
    "pair = max(c.items(), key=lambda x: (x[1], x[0]))[0]\n",
    "new_token = b\"\".join(pair)\n",
    "new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31116e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 101, 108, 108, 111)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(b\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b6c030c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes((104))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4587c70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046a74a",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f35a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1270e+21,  4.5590e-41],\n",
       "         [ 1.1270e+21,  4.5590e-41],\n",
       "         [ 2.0208e+23,  3.0767e-41],\n",
       "         [ 2.0207e+23,  3.0767e-41]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.9828e+23,  3.0767e-41],\n",
       "         [ 2.1456e+22,  3.0767e-41],\n",
       "         [-5.5259e+03,  4.5587e-41]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.4013e-45,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 9.8091e-45,  0.0000e+00]],\n",
       "\n",
       "        [[ 1.4013e-45,  0.0000e+00],\n",
       "         [-1.7014e+38,  1.1515e-40],\n",
       "         [ 4.5919e-41,  4.1478e-43],\n",
       "         [ 3.8255e-43,  0.0000e+00]],\n",
       "\n",
       "        [[ 1.1269e+21,  4.5590e-41],\n",
       "         [ 1.1269e+21,  4.5590e-41],\n",
       "         [ 1.9733e+23,  3.0767e-41],\n",
       "         [ 2.0204e+23,  3.0767e-41]],\n",
       "\n",
       "        [[ 1.9925e+23,  3.0767e-41],\n",
       "         [ 2.1456e+22,  3.0767e-41],\n",
       "         [-5.5252e+03,  4.5587e-41],\n",
       "         [ 1.4013e-45,  2.1019e-44]],\n",
       "\n",
       "        [[ 1.4013e-45, -1.7014e+38],\n",
       "         [ 1.1515e-40,  4.5919e-41],\n",
       "         [ 4.1478e-43,  2.0319e-43],\n",
       "         [ 1.1269e+21,  4.5590e-41]],\n",
       "\n",
       "        [[ 1.1269e+21,  4.5590e-41],\n",
       "         [ 1.9977e+23,  3.0767e-41],\n",
       "         [ 2.0007e+23,  3.0767e-41],\n",
       "         [ 1.9878e+23,  3.0767e-41]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2],\n",
       "        [ 3,  4],\n",
       "        [ 5,  6],\n",
       "        [ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for dimension 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m display(pos.shape)\n\u001b[32m      7\u001b[39m display(pos)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m res = \u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(res.shape)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m res.shape == pos.shape + cache.shape[\u001b[32m1\u001b[39m:]\n",
      "\u001b[31mIndexError\u001b[39m: index 10 is out of bounds for dimension 0 with size 10"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cache = torch.empty(10, 4, 2)\n",
    "display(cache)\n",
    "pos = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 2]])\n",
    "display(pos.shape)\n",
    "display(pos)\n",
    "res = cache[pos]\n",
    "print(res.shape)\n",
    "assert res.shape == pos.shape + cache.shape[1:]\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92134d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-4.8529e-01,  2.6964e-01, -9.3886e-02, -9.0110e-02,  3.7214e-01,\n",
       "                       -8.5903e-02, -1.6224e-01, -6.9695e-02,  3.8484e-01,  5.7567e-01,\n",
       "                        5.5860e-02, -2.9090e-02, -3.8228e-01,  2.7765e-01, -1.4191e-01,\n",
       "                       -2.8157e-01,  3.6442e-02,  3.6201e-01,  1.3592e-01,  5.2417e-01],\n",
       "                      [ 2.8679e-01,  2.6349e-01, -2.2492e-01, -1.6420e-01,  2.7580e-01,\n",
       "                        5.5705e-02, -2.6432e-01,  6.8026e-02, -2.8690e-02, -2.4032e-02,\n",
       "                        3.6914e-03, -6.3540e-02,  1.0283e-04,  9.1783e-02,  2.2163e-01,\n",
       "                        8.8964e-02, -1.1619e-01, -1.9740e-01,  7.6988e-02, -4.1063e-01],\n",
       "                      [-3.0441e-01,  1.7776e-01,  1.6846e-01,  6.0462e-01, -2.5959e-01,\n",
       "                        2.7505e-01, -5.2852e-01, -3.4432e-01,  2.8347e-01, -1.1309e-01,\n",
       "                       -5.5541e-02,  2.3649e-01,  7.9557e-02,  6.8420e-01, -1.9286e-01,\n",
       "                        1.2384e-01,  2.6129e-01, -3.1309e-01, -1.8562e-02,  3.0621e-01],\n",
       "                      [ 1.0690e-01,  2.0866e-01, -2.3433e-01, -1.9442e-02,  4.0275e-01,\n",
       "                        2.8823e-01, -2.7568e-02, -7.3719e-02, -1.2860e-01,  6.2770e-02,\n",
       "                        1.0868e-01,  2.1072e-01, -1.9919e-01, -4.1084e-01, -3.7275e-01,\n",
       "                       -2.6746e-01, -6.7860e-02,  1.1613e-01,  8.2715e-02, -3.5957e-01],\n",
       "                      [ 5.9840e-02,  7.6832e-02, -4.4755e-01, -6.5624e-01,  7.4615e-03,\n",
       "                       -1.2729e-01, -2.5106e-01,  6.6763e-01, -6.9866e-01,  6.4044e-02,\n",
       "                       -4.4586e-02, -6.5706e-02, -2.5661e-01, -5.2745e-01,  3.7660e-01,\n",
       "                       -2.2321e-02,  4.8739e-01, -3.6195e-02,  3.1021e-01,  5.8044e-02]])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0385, -0.1934,  0.0978, -0.2619, -0.1381],\n",
       "                      [-0.5289,  0.2469, -0.6085,  0.1976,  0.6620],\n",
       "                      [-0.0352, -0.0616,  0.0273,  0.4919, -0.2409],\n",
       "                      [ 0.1483, -0.3140, -0.0990,  0.4882, -0.0212],\n",
       "                      [ 0.0429,  0.0914,  0.1202, -0.0896, -0.4474],\n",
       "                      [ 0.0243, -0.4912, -0.0962,  0.3576,  0.1709],\n",
       "                      [ 0.5985,  0.1138, -0.4801,  0.1144, -0.1587],\n",
       "                      [ 0.0151,  0.1068,  0.0192, -0.0749, -0.5878],\n",
       "                      [-0.1315,  0.3725, -0.2751,  0.1196,  0.1610],\n",
       "                      [-0.2809,  0.2354, -0.2845, -0.0015, -0.1084],\n",
       "                      [-0.4323,  0.2186,  0.2302, -0.6999,  0.1408],\n",
       "                      [ 0.5436,  0.1901,  0.0558,  0.1743, -0.0605],\n",
       "                      [ 0.1494,  0.2107,  0.0607,  0.1194, -0.0334],\n",
       "                      [ 0.0849, -0.3901,  0.0666,  0.2275,  0.2132],\n",
       "                      [-0.1969, -0.3043, -0.6479,  0.4155,  0.1654],\n",
       "                      [-0.2131, -0.0042,  0.1059, -0.1516, -0.3541],\n",
       "                      [ 0.1711, -0.5155,  0.6119,  0.6180, -0.2358],\n",
       "                      [-0.4554,  0.1952,  0.6079, -0.2815, -0.4018],\n",
       "                      [ 0.0129, -0.1458, -0.0344, -0.0724, -0.6445],\n",
       "                      [-0.1810, -0.2058, -0.1873,  0.1930,  0.0884]])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-0.1750,  0.0971,  0.0761,  0.0482, -0.6799, -0.5692,  0.0911,  0.1610,\n",
       "                       -0.2135, -0.0511, -0.3297, -0.2657, -0.1494,  0.2699, -0.1743, -0.1223,\n",
       "                       -0.2771,  0.0205, -0.2023, -0.0600],\n",
       "                      [-0.2412,  0.0088,  0.1568, -0.2503, -0.7378,  0.2836,  0.3339, -0.5158,\n",
       "                       -0.3562,  0.2278,  0.0143, -0.2090, -0.1547, -0.1264,  0.0497, -0.7350,\n",
       "                        0.3080, -0.1599, -0.6907, -0.4788],\n",
       "                      [-0.2270, -0.0842, -0.0702,  0.0209, -0.1592, -0.3454, -0.2198,  0.4561,\n",
       "                       -0.1657, -0.3517, -0.5264,  0.1053, -0.2506, -0.2724, -0.3547, -0.6064,\n",
       "                        0.2973,  0.0838, -0.0115, -0.3904],\n",
       "                      [-0.0549, -0.2248, -0.2030,  0.1273,  0.1327,  0.2441, -0.4228, -0.2286,\n",
       "                       -0.1304,  0.0845,  0.3802,  0.1125, -0.4648, -0.1071,  0.0721,  0.0788,\n",
       "                        0.6674,  0.3592, -0.3883, -0.0479],\n",
       "                      [ 0.3873, -0.1248, -0.0382, -0.1150, -0.0789,  0.0987, -0.2014,  0.0407,\n",
       "                        0.0288,  0.1926, -0.4739, -0.2145,  0.1803, -0.2898, -0.3366, -0.2805,\n",
       "                        0.0230, -0.1207,  0.3107, -0.3810]]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cs336_basics.modules import SwiGLU\n",
    "\n",
    "m = SwiGLU(5, 20)\n",
    "m.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336",
   "language": "python",
   "name": "cs336"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
